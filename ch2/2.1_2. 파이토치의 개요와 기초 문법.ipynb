{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cbdd7d",
   "metadata": {},
   "source": [
    "2.1 파이토치 개요\n",
    "1) 파이토치의 특징 및 장점 : GPU에서 텐서 조작 및 동적 신경망 구축이 가능한 프레임 워크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6fdc0",
   "metadata": {},
   "source": [
    "*note* 벡터. 행렬. 텐서\n",
    "1. 1차원 축(행) : 벡터\n",
    "2. 2차원 축(열) : 행렬\n",
    "3. 3차원 축(채널) : 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f28d42",
   "metadata": {},
   "source": [
    "파이토치 API\n",
    "1. torch : GPU를 지원하는 텐서 패키지\n",
    "2. torch.autograd : 자동미분 패키지\n",
    "3. torch.nn : 신경망 구축 및 훈련 패키지\n",
    "4. torch.multiprocessing : 파이썬 멀티프로세싱 패키지\n",
    "5. torch.utils : DataLoader 및 기타 유틸리티를 제공하는 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb08c9",
   "metadata": {},
   "source": [
    "2.2 파이토치 기초 문법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ffc24",
   "metadata": {},
   "source": [
    "텐서 생성 및 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f36121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.tensor([[1,2],[3,4]])) #2차원 형태의 텐서 생성\n",
    "#print(torch.tensor([[1,2],[3,4]], device=\"cuda:0\")) #GPU에 텐서 생성\n",
    "print(torch.tensor([[1,2],[3,4]], dtype=torch.float64)) #dtype을 이용하여 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6485ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(2.) tensor(7.)\n",
      "tensor([3., 4., 5.]) tensor([5., 6.])\n"
     ]
    }
   ],
   "source": [
    "temp = torch.FloatTensor([1,2,3,4,5,6,7])\n",
    "print(temp[0],temp[1],temp[-1]) #인덱스로 접근\n",
    "print(temp[2:5],temp[4:-1]) #슬라이스로 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c688ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1,2,3])\n",
    "w = torch.tensor([3,4,6])\n",
    "print(w-v) #길이가 같은 벡터간 뺄셈 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9316990b",
   "metadata": {},
   "source": [
    "2) 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단순한 파일을 불러와서 사용\n",
    "import pandas as pd\n",
    "immport torch\n",
    "data = pd.read_csv('car_evaluation.csv')\n",
    "x=torch.from_numpy(data['x'].values).unsqueeze(dim=1).float() #csv 파일의 x 칼럼의 값을 넘파이 배열로 받아 tensor로 바꾼다\n",
    "y=torch.from_numpy(data['y'].values).unsqueeze(dim=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#커스텀 데이터셋을 만들어서 사용\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self): #필요한 변수를 선언하고 데이터 셋의 전처리를 해주는 함수\n",
    "    def __len__(self): #데이터셋의 길이, 즉 총 샘플의 수를 가져오는 함수\n",
    "    def __getitem__(self,index) : #데이터셋에서 특정 데이터를 가져오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc1d0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파이토치에서 제공하는 데이터셋 사용\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(1.0,))\n",
    "]) #평균이 0.5, 표준편차가 1.0이 되도록 데이터의 분포를 조정\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b382ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /data/MNIST_DATASET\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 9912422/9912422 [00:01<00:00, 8522971.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST_DATASET\\MNIST\\raw\\train-images-idx3-ubyte.gz to /data/MNIST_DATASET\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /data/MNIST_DATASET\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 14450160.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST_DATASET\\MNIST\\raw\\train-labels-idx1-ubyte.gz to /data/MNIST_DATASET\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /data/MNIST_DATASET\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 4985353.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST_DATASET\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to /data/MNIST_DATASET\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /data/MNIST_DATASET\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 4533681.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST_DATASET\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to /data/MNIST_DATASET\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_root='/data/MNIST_DATASET' #내려받을 경로 지정\n",
    "\n",
    "train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
    "valid_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)\n",
    "test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20ef65",
   "metadata": {},
   "source": [
    "3. 모델 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a7749",
   "metadata": {},
   "source": [
    "- 계층 : 모듈 또는 모듈을 구성하는 한 개의 계층으로 합성곱층, 선형 계층 등이 있음\n",
    "- 모듈 : 한 개 이상의 계층이 모여서 구성된 것. 모듈이 모여 새로운 모듈을 만들 수 있음\n",
    "- 모델 : 최종적으로 원하는 네트워크. 한 개의 모듈이 모델이 될 수도 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf440fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단순 신경망을 정의하는 방법\n",
    "import torch.nn as nn\n",
    "model = nn.Linear(in_features=1, out_features=1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9546724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Module()을 상속하여 정의하는 벙법\n",
    "class MLP(ModuleName):\n",
    "    def __init__(self, inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer=Linear(inputs, 1)\n",
    "        self.activation = Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "        X = self.activation(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37657d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing childeren\n",
      "-----------------------\n",
      "[Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), Sequential(\n",
      "  (0): Conv2d(64, 30, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=750, out_features=10, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")]\n",
      "\n",
      "\n",
      "Printing Modules\n",
      "---------------------\n",
      "[MLP(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 30, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=750, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Sequential(\n",
      "  (0): Conv2d(64, 30, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), Conv2d(64, 30, kernel_size=(5, 5), stride=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Sequential(\n",
      "  (0): Linear(in_features=750, out_features=10, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "), Linear(in_features=750, out_features=10, bias=True), ReLU(inplace=True)]\n"
     ]
    }
   ],
   "source": [
    "#Sequential 신경망을 정의하는 방법\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=30, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(in_features=30*5*5, out_features=10, bias=True),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.layers1(x)\n",
    "            x = self.layers2(x)\n",
    "            x = x.view(x.shape[0],-1)\n",
    "            x = slf.layer3(x)\n",
    "            return x\n",
    "        \n",
    "model = MLP()\n",
    "\n",
    "print(\"Printing childeren\\n-----------------------\")\n",
    "print(list(model.children())) #class MLP 아래 수준의 노드 반환\n",
    "print(\"\\n\\nPrinting Modules\\n---------------------\")\n",
    "print(list(model.modules()))\n",
    "\n",
    "#nn.Sequential은 모델의 계층이 복잡할수록 효과가 뛰어나다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8c06a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#함수로 신경망을 정의\n",
    "#재사용 할 수 있는 장점 있지만 모델이 복잡해진다는 단점도 있음\n",
    "\n",
    "def MLP(in_features=1, hidden_features=20, out_features=1):\n",
    "    hidden = nn.Linear(in_features=in_features, out_features=hidden_features, bias=True)\n",
    "    activation = nn.ReLU()\n",
    "    output = nn.Linear(in_features=hidden_features, out_feature=out_features, bias=True)\n",
    "    net = nn.Sequential(hiddem, activation, output)\n",
    "    return net\n",
    "\n",
    "#ReLU, Softmax 및 Sigmoid와 같은 활성화 함수는 모델을 정의할 때 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1a577",
   "metadata": {},
   "source": [
    "4. 모델의 파라미터 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2587376",
   "metadata": {},
   "source": [
    "1. 손실함수 : 학습하는 동안 출력과 실제 값 사이의 오차를 측정 \n",
    "  - BCELoss : 이진분류\n",
    "  - CrossEntropyLoss : 다중클래스 분류\n",
    "  - MSELoss : 회귀모델\n",
    "2. 옵티마이저 : 데이터와 손실 함수를 바탕으로 모델의 업데이트 방법을 결정. step() 메소드를 통해 전달받은 파라미터 업데이트.\n",
    "  - torch.optim.Optimizer(params, defaults): 모든 옵티마이저의 기본이 되는 클래스\n",
    "  - zero_grad() : 옵티마이저에 사용된 파라미터들의 기울기를 0으로 만듬\n",
    "  - torch.optim.lr_scheduler : 에포크에 따라 학습률을 조정할 수 있음\n",
    "  - 옵티마이저의 종류: Adadelta, Adagrad, Adam, SparesAdam, Adamx, ASGD, LBFGS, RMSProp, Rprop, SGD\n",
    "3. 학습률 스케줄러 : 미리 지정한 횟수의 에포크를 지날 때마다 학습률을 감소. 최적점을 찾아갈 수 있게 해준다\n",
    "  - optim.lr_scheduler.LambdaLR : 람다 함수를 이용하여 결과를 학습률로 설정\n",
    "  - optim.lr_scheduler.StepLR : 특정 단계마다 학습률을 감마 비율만큼 감소\n",
    "  - optim.lr_scheduler.ExponentialLR : 에포크마다 이전 학습률에 감마만큼 곱함\n",
    "  - optim.lr_scheduler.CosineAnnealingLR : 학습률을 코사인 함수의 형태처럼 변화. 학습률이 커지기도 작아지기도 함\n",
    "  - optim.lr_scheduler.ReduceLROnPlateau : 학습이 잘 되고 있는지 아닌지에 따라 동적으로 학습률을 변화\n",
    "4. 지표 : 훈련과 테스트 단계를 모니터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86618120",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLambdaLR(optimizer\u001b[38;5;241m=\u001b[39moptimizer, lr_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch: \u001b[38;5;241m0.95\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mepoch)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m): \u001b[38;5;66;03m#에포크수만큼 데이터를 반복하여 처리\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m : \u001b[38;5;66;03m#배치 크기만큼 데이터를 가져와서 학습 진행\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m loss_fn(model(x),y)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "#모델의 파라미터를 정의하는 예시 코드\n",
    "import torch\n",
    "from torch.optim import optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95**epoch)\n",
    "for epoch in range(1, 100+1): #에포크수만큼 데이터를 반복하여 처리\n",
    "    for x, y in dataloader : #배치 크기만큼 데이터를 가져와서 학습 진행\n",
    "        optimizer.zero_grad()\n",
    "loss_fn(model(x),y).backward()\n",
    "optimizer.step()\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c3b47",
   "metadata": {},
   "source": [
    "5. 모델훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcdaf6",
   "metadata": {},
   "source": [
    "파이토치 학습 절차\n",
    "- 모델, 손실 함수, 옵티마이저 정의\n",
    "- optimizer.zero_grad(): 전방향 학습, 기울기 초기화\n",
    "- output = model(input) : 출력계산\n",
    "- loss = loss_fn(output, target) : 오차 계산\n",
    "- loss.backward() : 역전파 학습\n",
    "- optimizer.step() : 기울기 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89434bcc",
   "metadata": {},
   "source": [
    "6. 모델평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a66aeefc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "accuracy() missing 1 required positional argument: 'task'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m5\u001b[39m, (\u001b[38;5;241m10\u001b[39m,))\n\u001b[1;32m----> 7\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtorchmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: accuracy() missing 1 required positional argument: 'task'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "#원래 코드\n",
    "preds = torch.randn(10, 5).softmax(dim=-1)\n",
    "target = torch.randint(5, (10,))\n",
    "\n",
    "acc = torchmetrics.functional.accuracy(preds, target) #모델평가를 위해 torchmetrics, functional.accuracy 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "747cf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "preds = torch.randn(10, 5).softmax(dim=-1)\n",
    "target = torch.randint(5, (10,))\n",
    "\n",
    "acc = torchmetrics.functional.accuracy(preds, target, task='multiclass', num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b5a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모듈을 이용하여 모델을 평가하는 코드\n",
    "####원래 코드###\n",
    "import torch\n",
    "import torchmetrics\n",
    "metric = torchmetrics.Accuracy()\n",
    "\n",
    "n_batches = 10\n",
    "for i in range(n_batches):\n",
    "    preds = torch.rand(10,5).softmax(dim=-1)\n",
    "    target = torch.metrics(5,(10,))\n",
    "    \n",
    "    acc = metric(preds, target)\n",
    "    print(f\"Accuracy on batch : {i} : {acc}\") #현재 배치에서 모델평가\n",
    "    \n",
    "acc = metric.compute()\n",
    "print(f\"Accuracy on all data: {acc}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "041355d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on batch 0: 0.0\n",
      "Accuracy on batch 1: 0.30000001192092896\n",
      "Accuracy on batch 2: 0.10000000149011612\n",
      "Accuracy on batch 3: 0.5\n",
      "Accuracy on batch 4: 0.10000000149011612\n",
      "Accuracy on batch 5: 0.10000000149011612\n",
      "Accuracy on batch 6: 0.10000000149011612\n",
      "Accuracy on batch 7: 0.30000001192092896\n",
      "Accuracy on batch 8: 0.20000000298023224\n",
      "Accuracy on batch 9: 0.20000000298023224\n",
      "Accuracy on all data: 0.1899999976158142\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "num_classes = 5\n",
    "metric = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "n_batches = 10\n",
    "for i in range(n_batches):\n",
    "    preds = torch.rand(10, 5).softmax(dim=-1)\n",
    "    target = torch.randint(num_classes, (10,))\n",
    "    \n",
    "    acc = metric(preds, target)\n",
    "    print(f\"Accuracy on batch {i}: {acc.item()}\")  # Note: using `acc.item()` to get a scalar value\n",
    "    \n",
    "acc = metric.compute()\n",
    "print(f\"Accuracy on all data: {acc.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97193f4",
   "metadata": {},
   "source": [
    "7. 훈련 과정 모니터링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb969b23",
   "metadata": {},
   "source": [
    "1) 텐서보드 설정\n",
    "2) 텐서보드에 기록\n",
    "3) 텐서보드를 사용하여 모델 구조를 살펴본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c761ce5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m#학습 모드로 전환 (dropout=True)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdataloader\u001b[49m):\n\u001b[0;32m     10\u001b[0m     x,y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat(), y\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(x)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"/ch2\")\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() #학습 모드로 전환 (dropout=True)\n",
    "    batch_loss = 0.0\n",
    "    \n",
    "    for i, (x,y) in enumerate(dataloader):\n",
    "        x,y = x.to(device).float(), y.to(device).float()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        writer.add_scalar(\"Loss\",loss,epoch) #스칼라 값(오차)를 기록\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "writer.close() #SummaryWriter가 더 이상 필요하지 않으면 close() 메서드 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f869f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
